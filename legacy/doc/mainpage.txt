/**
@mainpage OFF

@author    Stefano Zaghi
@version   0.0.5
@date      2012-04-24
@copyright GNU Public License version 3.

@section Introduction

@off is an Open source Finite volumes Fluid dynamics code.

It is written in in standard (compliant) Fortran 2003 with highly modularity as design target.

The aim of @off is to solve, numerically, the compressible Navier-Stokes equations of fluid dynamics (\ref Equations "see governing equations") by means of Finite Volumes techniques. The main features of @off code are the following:
- Finite Volume, Godunov-like scheme based on Euler conservation Laws written in fully conservative formulation:
  - the extension to viscous Navier-Stokes equations is under developing;
- Underling Riemann Problem solver for convective fluxes:
  - Approximate Riemann solver based on (local) Lax-Friedrichs (known also as Rusanov) algorithm;
  - Approximate Riemann solver based on Primitive Variables Linearization algorithm;
  - Approximate Riemann solver based on Two Rarefactions algorithm;
  - Approximate Riemann solver based on Two Shocks algorithm;
  - Approximate Riemann solver based on Adaptive (non iterative) PVL-TR-TS algorithm;
  - Approximate Riemann solver based on Adaptive (non iterative) LF-TR algorithm;
  - Approximate Riemann solver based on HLLC algorithm;
  - Approximate Riemann solver based on Roe linearization.
  - Exact Riemann solver based on iterative solution of u-function;
- Multi-Species fluids models:
  - Partial Densities species conservation (Standard Thermodynamic Model);
  - New multi-dimensional \f$\gamma,\eta,\chi\f$ conservation models of Favini, B. et al (under developing);
- Multi-Phases fluids models:
  - Fully-coupled Lagrangian particles transport model (under developing);
- Space numerical integration models:
  - \f$1^{st}\f$ order piece-wise constant reconstruction;
  - \f$2^{nd}\f$ order TVD linear-wise reconstruction;
  - \f$3^{rd}\f$,\f$5^{th}\f$,\f$7^{th}\f$ order WENO non-linear reconstruction;
- Time approximation models:
  - \f$1^{st}\f$ order forward Euler integration;
  - \f$2^{nd}\f$,\f$3^{rd}\f$,\f$4^{th}\f$ order Strong-Stability-Preserving explicit Runge-Kutta integration;
- Local pseudo-time convergence acceleration for steady simulations;
- Multi-grid time convergence acceleration:
  - Multi-grid model has been already developed, but it is affected by some not yet recognized bugs. Testing and bugs fixing are in progress.
- Underling numerical grid models:
  - 3D, general curvilinear, body-fitted, structured multi-blocks mesh;
  - Adaptive Mesh Refinement, AMR model (under developing);
  - Blocks overlapping, overset (Chimera) model (to be developed in future);
- Computational parallelism ability:
  - Domain decomposition by means of Message Passing Interface (MPI) paradigm providing the ability to use distributed-memory cluster facilities;
  - Fine, local parallelism by means of OpenMP paradigm providing the ability to use shared-memory cluster facilities;
  - Fine, local parallelism by means of GPU programming (e.g. CUDA framework) providing the ability to use GPUs cluster facilities (to be developed in future).

@note
<b>Compiling Instructions</b> \n
@off is shipped with a makefile for compiling the codes on Unix/GNU Linux architectures. Other OS are not supported. For more details see \ref Compiling "Compiling Instructions".

@bug <b>Multi-grid Models</b>: \n Multi-grid time convergence acceleration has been developed, but it is affected by some bugs
                               that <em>blow up</em> steady simulations.
@todo \b MultiSpeciesModel: Introducing new multi-dimensional \f$\gamma,\eta,\chi\f$ conservation models of Favini, B. et al
@todo \b MultiPhaseModel: Introducing fully-coupled Lagrangian particles transport model
@todo \b AMR: Introducing AMR (Adaptive Mesh Refinement) model
@todo \b Chimera: Introducing blocks overlapping, overset (Chimera) model
@todo \b GPU: Introducing fine, local parallelism by means of GPU programming (e.g. CUDA framework)
@todo \b DocImprove: Improve the documentation

@subsection AuxCodes Auxiliary Codes

There are two useful auxiliary codes:
-  IBM: Initial and Boundary conditions, Mesh generator for @off;
-  POG: Post-processing Output Generator for @off.

@section IBM

It is an Initial and Boundary conditions, Mesh generator for @off.

This is an auxiliary tool useful for building proper inputs for @off code. ICG can build Initial Conditions files, Mesh files and Boundary Conditions files. It accepts two kinds of inputs:
- Direct Blocks Description: this is the simplest available input. The initial and boundary descriptions as well as the geometry are directly described by means of simple ascii files. This kind of inputs can describe only simple Cartesian grids.
- Ansys (http://www.ansys.com) IcemCFD Multiblock INFO importer: this is a more complex (but more flexible) input. The initial conditions are described by means of simple ascii files similar to the Direct Block Description input, but the boundary conditions and the geometry are loaded by Ansys IcemCFD Multiblock INFO files. These files can describe more complex scenario with general curvilinear grids.

@section POG

It is a Post-processor Output Generator for @off.

This is an auxiliary tool useful for post-processing @off simulations outputs. It can manipulate @off outputs and it can produce files ready to be visualized. Two different visualization standards are supported:
- Tecplot, Inc.: Tecplot is a wide-used visualization tool (http://www.tecplot.com/). POG can produce both ascii and binary files in Tecplot standard.
- VTK: The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization (http://www.vtk.org/). A lot of visualization tools support VTK standard. Among those tools Paraview (http://www.paraview.org/) seems to be one of the most complete. POG can produce both ascii and binary files in VTK standard. To this aim the Lib_VTK_IO is used.

@section Copyrights

@off is an open source project, it is distributed under the GPL v3. Anyone is interest to use, to develop or contribute to
@off is welcome.

@section Source Source Code

It can be found at: https://github.com/szaghi/OFF

@page Equations Governing Equations

Description of the main mathematical models

@section conseq Conservation Laws

The turbulent motion of compressible viscous fluid is described by a system of equations, that read, in integral, dimensional form:

\f$\frac{\partial }{{\partial t}}\int\limits_V {\overrightarrow U dV}  + \oint\limits_{S(V)} {\overline{\overline F}  \cdot \overrightarrow n dS}  = \int\limits_V {\overrightarrow {{Q_V}} dV}  + \oint\limits_{S(V)} {\overline{\overline {{Q_S}}} \cdot \overrightarrow n dS} \f$ \n
\f$V\f$ being the control volume, \f$S(V)\f$ its boundary, \f$\vec n\f$ the outward unit normal of \f$S(V)\f$ and where \f$\overrightarrow U \f$ is the vector of conservative variables, \f$\overline{\overline F}\f$ is the conservative fluxes tensor, \f$\overrightarrow{Q_V}\f$ is vector of volume source terms and \f$\overline{\overline{Q_S}}\f$ is the tensor of surface source terms.

For fluid subjected to external forces and heat sources the above vectors and tensors could be written as:

\f$\overrightarrow U  = \left[ {\begin{array}{*{20}{c}} \rho \\ {\rho \vec v}\\ {\rho E} \end{array}} \right]\f$ \ref data_type_conservative::type_conservative "see definition of conservative derived type".


\f$\overline{\overline F}  - \overline{\overline {{Q_S}}}  = \left[ {\begin{array}{*{20}{c}} {\rho \vec v}\\ {\rho \vec v\vec v - \overline{\overline \sigma } }\\ {\rho E\vec v - \overline{\overline \sigma }  \cdot \vec v - k\overrightarrow \nabla  T} \end{array}} \right]\f$

\f$\overrightarrow {{Q_V}}  = \left[ {\begin{array}{*{20}{c}} 0\\ {\rho \overrightarrow {{f_e}} }\\ {\rho \overrightarrow {{f_e}} \cdot \vec v + {q_h}} \end{array}} \right]\f$ \n
where:

 - \f$\rho\f$ is the density;
 - \f$\vec v\f$ is the velocity vector;
 - \f$\vec v \vec v\f$ is the dyadic product of velocity vector defined as
    \f$\vec u\vec v = \left[ {\begin{array}{*{20}{c}}
      {{u_1}{v_1}} & {{u_1}{v_2}} & {{u_1}{v_3}}  \\ {{u_2}{v_1}} & {{u_2}{v_2}} & {{u_2}{v_3}}  \\ {{u_3}{v_1}} & {{u_3}{v_2}} & {{u_3}{v_3}}
    \end{array}} \right]\f$
 - \f$E\f$ is the specific, total energy;
 - \f$T\f$ is the absolute temperature;
 - \f$\overline{\overline \sigma}\f$ is the total internal stress tensor;
 - \f$k\f$ is the thermal conductivity coefficient,  \f$-k\overrightarrow \nabla  T\f$ being the Fourier's law of heat conduction; the molecular diffusion and the radiative heat transfer have been neglected;
 - \f$\overrightarrow{{f_e}}\f$ is the external specific volume forces vector;
 - \f$q_h\f$ is heat sources other than conduction.

The total internal stress could be written as (constitutive law):

\f$ \overline{\overline \sigma} = -p\overline{\overline I}+ \overline{\overline \tau}\f$ \n
where:
 - \f$p\f$ is the static, isotropic pressure (\f$\overline{\overline I}\f$ is the identity tensor);
 - \f$\overline{\overline \tau}\f$ is the viscous shear stress tensor.

The viscous shear stress tensor, in general case, is defined as:

\f${\tau _{ij}} = \mu \left( {{\partial _i}{v_j} + {\partial _j}{v_i}} \right) + \lambda \left( {\overrightarrow \nabla   \cdot \vec v} \right){\delta _{ij}}\f$ \n
where:
 - \f$\mu\f$ is the dynamic viscosity of the fluid;
 - the second viscosity is defined as \f$\lambda = -\frac{2}{3}\mu\f$ for a Newtonian fluid in local thermodynamic equilibrium (except very high temperature or pressure ranges); this is the Stokes hypothesis;
 - \f$\delta_{ij}\f$ is the Kronecker delta \f${\delta _{ij}} = \left\{ {\begin{array}{*{20}{c}} {0{\rm{,}}\;{\rm{if}}\;i \ne j}\\ {{\rm{1,}}\;{\rm{if}}\;i = j} \end{array}} \right.\f$.

In tensorial form the shear stress tensor could be written as:

\f$\overline{\overline \tau }  = 2\mu \frac{1}{2}\left( {\overrightarrow \nabla  \vec v + \overrightarrow \nabla  {{\vec v}^T}} \right) + \lambda \left( {\overrightarrow \nabla   \cdot \vec v} \right)\overline{\overline I} \f$ \n
where \f$ \frac{1}{2}\left( {\overrightarrow \nabla  \vec v + \overrightarrow \nabla  {{\vec v}^T}} \right)\f$ is the symmetric part of the velocity vector gradient.

The above system of equation must be completed by the constitutive equation of state. For a perfect gas, i.e. thermally and
calorically perfect gas, the equation of state is:

\f$ p = \rho R T\f$ \n
where \f$R\f$ is the gas constant. This constant is related to the specific heats at constant volume and constant pressure (and their ratio):

\f$\begin{array}{*{20}{c}}\gamma = \frac{c_p}{c_v} \\ R = c_p -c_v  \\ c_p = \frac{\gamma R}{\gamma -1} \\ c_p = \frac{R}{\gamma -1} \end{array}\f$

The other equations of state are:

 - \f$ E = c_v T + \frac{|\vec v|^2}{2} = \frac{p}{\rho(\gamma-1)} + \frac{|\vec v|^2}{2}\f$ is the total specific energy definition;
 - \f$ a = \sqrt{\frac{\gamma p}{\rho}} \f$ is the speed of sound (acoustic velocity);
 - \f$ H = c_p T + \frac{|\vec v|^2}{2} = \frac{\gamma p}{\rho(\gamma-1)} + \frac{|\vec v|^2}{2}= \frac{a^2}{\gamma-1} + \frac{|\vec v|^2}{2}\f$ is the total specific enthalpy definition.

@subsection adim Non-Dimensional Form

In order to derive the non-dimensional form let us introduce non-dimensional quantities \f$x'=\frac{x}{x_0}\f$ being \f$x\f$ dimensional quantity and \f$x_0\f$ an arbitrary reference value. With this nomenclature using a dimensional analysis the above equations could be written as:

\f$ \begin{array}{l} \frac{{{\rho _0}L_0^3}}{{{t_0}}}\frac{\partial }{{\partial t'}}\int\limits_{V'} {\rho 'dV'}  + {\rho _0}{v_0}L_0^2\oint\limits_{S'(V')} {\rho '\overrightarrow {v'}  \cdot \vec ndS'}  = 0\\ \frac{{{\rho _0}{v_0}L_0^3}}{{{t_0}}}\frac{\partial }{{\partial t'}}\int\limits_{V'} {\rho '\overrightarrow {v'} dV'}  + {\rho _0}v_0^2L_0^2\oint\limits_{S'(V')} {\rho '\overrightarrow {v'} \overrightarrow {v'}  \cdot \vec ndS'}  + {p_0}L_0^2\oint\limits_{S'(V')} {p'\overline{\overline I}  \cdot \vec ndS'}  - {\tau _0}L_0^2\oint\limits_{S'(V')} {\overline{\overline {\tau '}}  \cdot \vec ndS'}  = {\rho _0}{f_0}L_0^3\int\limits_{V'} {\rho '\overrightarrow {{{f'}_e}} dV'} \\ \frac{{{\rho _0}{E_0}L_0^3}}{{{t_0}}}\frac{\partial }{{\partial t'}}\int\limits_{V'} {\rho 'E'dV'}  + {\rho _0}{E_0}{v_0}L_0^2\oint\limits_{S'(V')} {\rho 'E'\overrightarrow {v'}  \cdot \vec ndS'}  + {p_0}L_0^2\oint\limits_{S'(V')} {p'\overrightarrow {v'}  \cdot \vec ndS'}  - {\tau _0}{v_0}L_0^2\oint\limits_{S'(V')} {\left( {\overline{\overline {\tau '}}  \cdot \overrightarrow {v'} } \right) \cdot \vec ndS'}  + \\ -\frac{{{k_0}{T_0}L_0^2}}{{{L_0}}}\oint\limits_{S'(V')} {k'\overrightarrow {\nabla '} T' \cdot \vec ndS'}  = {\rho _0}{f_0}{v_0}L_0^3\int\limits_{V'} {\rho '\overrightarrow {{{f'}_e}}  \cdot \overrightarrow {v'} dV'}  + {\rho _0}{q_0}L_0^3\int\limits_{V'} {\rho '{{q'}_h}dV'} \end{array} \f$

In order to make non-dimensional the above system divide for \f${\rho _0}{v_0}L_0^2\f$, \f${\rho _0}{v_0^2}L_0^2\f$ and \f${\rho _0}{E_0}{v_0}L_0^2\f$ the conservation equation of mass, momentum and energy respectively, obtaining:

\f$ \begin{array}{l} \frac{{{L_0}}}{{{v_0}{t_0}}}\frac{\partial }{{\partial t'}}\int\limits_{V'} {\rho 'dV'}  + \oint\limits_{S'(V')} {\rho '\overrightarrow {v'}  \cdot \vec ndS'}  = 0\\ \frac{{{L_0}}}{{{v_0}{t_0}}}\frac{\partial }{{\partial t'}}\int\limits_{V'} {\rho '\overrightarrow {v'} dV'}  + \oint\limits_{S'(V')} {\rho '\overrightarrow {v'} \overrightarrow {v'}  \cdot \vec ndS'}  + \frac{{{p_0}}}{{{\rho _0}v_0^2}}\oint\limits_{S'(V')} {p'\overline{\overline I}  \cdot \vec ndS'}  - \frac{{{\tau _0}}}{{{\rho _0}v_0^2}}\oint\limits_{S'(V')} {\overline{\overline {\tau '}}  \cdot \vec ndS'}  = \frac{{{f_0}{L_0}}}{{v_0^2}}\int\limits_{V'} {\rho '\overrightarrow {{{f'}_e}} dV'} \\ \frac{{{L_0}}}{{{v_0}{t_0}}}\frac{\partial }{{\partial t'}}\int\limits_{V'} {\rho 'E'dV'}  + \oint\limits_{S'(V')} {\rho 'E'\overrightarrow {v'}  \cdot \vec ndS'}  + \frac{{{p_0}}}{{{\rho _0}{E_0}{v_0}}}\oint\limits_{S'(V')} {p'\overrightarrow {v'}  \cdot \vec ndS'}  - \frac{{{\tau _0}}}{{{\rho _0}{E_0}}}\oint\limits_{S'(V')} {\left( {\overline{\overline {\tau '}}  \cdot \overrightarrow {v'} } \right) \cdot \vec ndS'}  + \\ - \frac{{{k_0}{T_0}}}{{{\rho _0}{E_0}{v_0}{L_0}}}\oint\limits_{S'(V')} {k'\overrightarrow {\nabla '} T' \cdot \vec ndS'}  = \frac{{{f_0}{L_0}}}{{{E_0}}}\int\limits_{V'} {\rho '\overrightarrow {{{f'}_e}}  \cdot \overrightarrow {v'} dV'}  + \frac{{{q_0}{L_0}}}{{{E_0}{v_0}}}\int\limits_{V'} {\rho '{{q'}_h}dV'} \end{array}\f$

It is possible to recognize 7 non-dimensional numbers:
 - \f$ \rm{St} = \frac{L_0}{v_0 t_0}\f$ Strouhal number; it is the ratio between characteristic frequency and the fluid dynamic one; for non periodic flow it is set to 1;
 - \f$ \rm{Ru} = \frac{\rho_0 v_0^2}{p_0}\f$ Ruark number; it is the ratio between inertial (convective) force and pressure one; for a Newtonian fluid it is set to 1;
 - \f$ \rm{Ma} = \frac{v_0}{a_0}\f$ Mach number; it is the ratio between velocity and speed of sound;
 - \f$ \rm{Re} = \frac{\rho_0 v_0 L_0}{\mu_0}\f$ Reynolds number; it is the ratio between inertial (convective) force and viscous one;
 - \f$ \rm{Fr} = \sqrt{\frac{v_0^2}{f_0 L_0}}\f$ Froude number; it is the ratio between inertial (convective) force and volume (mass) one;
 - \f$ \rm{Pr} = \frac{\mu_0 c_{p0}}{k_0}\f$ Prandtl number; it is the ratio between momentum diffusion and heat one;
 - \f$ \rm{Ec} = \frac{v_0^2}{c_{p0}T_0}\f$ Eckert number; it is the ratio between kinetic energy and enthalpy one;

By means of the above non-dimensional numbers the conservation equations could be written as:

\f$ \begin{array}{l} {\rm{St}}\frac{\partial }{{\partial t'}}\int\limits_{V'} {\rho 'dV'}  + \oint\limits_{S'(V')} {\rho '\overrightarrow {v'}  \cdot \vec ndS'}  = 0\\ {\rm{St}}\frac{\partial }{{\partial t'}}\int\limits_{V'} {\rho '\overrightarrow {v'} dV'}  + \oint\limits_{S'(V')} {\rho '\overrightarrow {v'} \overrightarrow {v'}  \cdot \vec ndS'}  + \frac{1}{{{\rm{Ru}}}}\oint\limits_{S'(V')} {p'\overline{\overline I}  \cdot \vec ndS'}  - \frac{1}{{{\rm{Re}}}}\oint\limits_{S'(V')} {\overline{\overline {\tau '}}  \cdot \vec ndS'}  = \frac{1}{{{\rm{F}}{{\rm{r}}^2}}}\int\limits_{V'} {\rho '\overrightarrow {{{f'}_e}} dV'} \\ {\rm{St}}\frac{\partial }{{\partial t'}}\int\limits_{V'} {\rho 'E'dV'}  + \oint\limits_{S'(V')} {\rho 'E'\overrightarrow {v'}  \cdot \vec ndS'}  + \frac{{{\rm{Ec}}}}{{{\rm{Ru}}}}\oint\limits_{S'(V')} {p'\overrightarrow {v'}  \cdot \vec ndS'}  - \frac{{{\rm{Ec}}}}{{{\rm{Re}}}}\oint\limits_{S'(V')} {\left( {\overline{\overline {\tau '}}  \cdot \overrightarrow {v'} } \right) \cdot \vec ndS'}  + \\ - \frac{1}{{{\rm{PrRe}}}}\oint\limits_{S'(V')} {k'\overrightarrow {\nabla '} T' \cdot \vec ndS'}  = \frac{{{\rm{Ec}}}}{{{\rm{F}}{{\rm{r}}^2}}}\int\limits_{V'} {\rho '\overrightarrow {{{f'}_e}}  \cdot \overrightarrow {v'} dV'}  + \frac{{{q_0}{L_0}}}{{{E_0}{v_0}}}\int\limits_{V'} {\rho '{{q'}_h}dV'} \end{array} \f$

We are interested in Newtonian fluid in which \f$\rm{St} = 1\f$, \f$\rm{Ru} = 1 => \rm{Ma} = \frac{1}{\sqrt{\gamma}}\f$ and \f$\rm{Ec} = 1\f$:
 - \f$ t_0 = \frac{L_0}{v_0}\f$
 - \f$ p_0 = \rho_0 v_0^2 => a_0 = \sqrt{\gamma}v_0\f$
 - \f$ c_{p0}T_0 = v_0^2\f$

As a consequence the non dimensional conservation equations are:

\f$ \begin{array}{l} \frac{\partial }{{\partial t'}}\int\limits_{V'} {\rho 'dV'}  + \oint\limits_{S'(V')} {\rho '\overrightarrow {v'}  \cdot \vec ndS'}  = 0\\ \frac{\partial }{{\partial t'}}\int\limits_{V'} {\rho '\overrightarrow {v'} dV'}  + \oint\limits_{S'(V')} {\left( {\rho '\overrightarrow {v'} \overrightarrow {v'}  + p'\overline{\overline I} } \right) \cdot \vec ndS'}  - \frac{1}{{{\rm{Re}}}}\oint\limits_{S'(V')} {\overline{\overline {\tau '}}  \cdot \vec ndS'}  = \frac{1}{{{\rm{F}}{{\rm{r}}^2}}}\int\limits_{V'} {\rho '\overrightarrow {{{f'}_e}} dV'} \\ \frac{\partial }{{\partial t'}}\int\limits_{V'} {\rho 'E'dV'}  + \oint\limits_{S'(V')} {\left( {\rho 'E' + p'} \right)\overrightarrow {v'}  \cdot \vec ndS'}  - \frac{{\rm{1}}}{{{\rm{Re}}}}\oint\limits_{S'(V')} {\left( {\overline{\overline {\tau '}}  \cdot \overrightarrow {v'} } \right) \cdot \vec ndS'}  - \frac{1}{{{\rm{PrRe}}}}\oint\limits_{S'(V')} {k'\overrightarrow {\nabla '} T' \cdot \vec ndS'}  = \\ \frac{{\rm{1}}}{{{\rm{F}}{{\rm{r}}^2}}}\int\limits_{V'} {\rho '\overrightarrow {{{f'}_e}}  \cdot \overrightarrow {v'} dV'}  + \frac{{{q_0}{L_0}}}{{{E_0}{v_0}}}\int\limits_{V'} {\rho '{{q'}_h}dV'} \end{array} \f$

The selected non-dimensional numbers are:
 - \f$ \rm{Re} = \frac{\rho_0 v_0 L_0}{\mu_0}\f$ Reynolds number;
 - \f$ \rm{Fr} = \sqrt{\frac{v_0^2}{f_0 L_0}}\f$ Froude number;
 - \f$ \rm{Pr} = \frac{\mu_0 c_{p0}}{k_0}\f$ Prandtl number;

The above non-dimensional numbers are dependent each other. In Reynolds and Froude numbers \f$L_0\f$ and \f$v_0\f$ are present.  Similarly in Reynolds and Prandtl numbers \f$\mu_0\f$ is present. Therefore fixing the above non-dimensional numbers the following reference values must be fixed:
 - \f$ L_0 \f$ reference length;
 - \f$ \rho_0 \f$ reference density;
 - \f$ v_0 \f$ reference velocity;
 - \f$ \mu_0 \f$ reference dynamic viscosity;
 - \f$ f_0 \f$ reference specific force;
 - \f$ c_0 \f$ reference specific heat;
 - \f$ k_0 \f$ reference thermal conductivity coefficient.

We have fixed the following reference values:
 - \f$ L_0 \f$ reference length;
 - \f$ \rho_0 \f$ reference density;
 - \f$ v_0 \f$ reference velocity;
 - \f$ c_0 \f$ reference specific heat.

These selections with the above selected non-dimensional numbers fix all other reference quantities:
 - \f$ \mu_0 = \frac{\rho_0 v_0 L_0}{\rm{Re}}\f$ reference dynamic viscosity;
 - \f$ f_0 = \frac{v_0^2}{L_0 \rm{Fr}^2}\f$ reference specific force;
 - \f$ k_0 = \frac{\mu_0 c_0}{\rm{Pr}}\f$ reference thermal conductivity coefficient.
 - \f$ t_0 = \frac{L_0}{v_0}\f$ reference time;
 - \f$ p_0 = \rho_0 v_0^2 \f$ reference pressure;
 - \f$ a_0 = \sqrt{\gamma}v_0 \f$ reference speed of sound;
 - \f$ T_0 = \frac{v_0^2}{c_0} \f$ reference temperature;
 - \f$ E_0 = v_0^2 \f$ reference specific energy;
 - \f$ q_0 = \frac{v_0^3}{L_0} \f$ reference specific heat.

The above equations constitute a system of partial differential equations closed by enforcing appropriate boundary conditions and initial conditions (at physical and computational boundaries).

@section NumericalModels Numerical Models

For the numerical solution of the above system a shock-capturing finite volume scheme is adopted. The fluid domain \f$D\f$ is decomposed in \f$N_b\f$ structured blocks \f$D^b\f$, each subdivided in \f$N_i \times N_j \times N_k\f$ disjoint hexahedrons \f$D_{ijk}^b\f$ such that \f$\bigcup D_{ijk}^b = D^b\f$. Conservation laws are then applied to each finite volume:

\f$\frac{\partial }{{\partial t}}\int\limits_{{V_{ijk}}} {\overrightarrow U d{V_{ijk}}}  + \sum\limits_{s = 1}^6 {\int\limits_{{S_s}\left( {{V_{ijk}}} \right)} {\overline{\overline F}  \cdot \overrightarrow {{n_s}} d{S_s}} }  = \int\limits_{{V_{ijk}}} {\overrightarrow {{Q_V}} d{V_{ijk}}}  + \sum\limits_{s = 1}^6 {\int\limits_{{S_s}\left( {{V_{ijk}}} \right)} {\overline{\overline {{Q_S}}}  \cdot \overrightarrow {{n_s}} d{S_s}} } \f$ \n
where \f$S_s\f$ is the \f$s^{th}\f$ face of the finite volume \f$D_{ijk}\f$ whose measure is \f$V_{ijk}\f$.

This system could be written in terms of \em residual \em function:

\f$\begin{array}{l} \frac{\partial }{{\partial t}}\int\limits_{{V_{ijk}}} {\overrightarrow U d{V_{ijk}}}  = R\left( {\overrightarrow U } \right)\\ R\left( {\overrightarrow U } \right) =  - \sum\limits_{s = 1}^6 {\int\limits_{{S_s}\left( {{V_{ijk}}} \right)} {\overline{\overline F}  \cdot \overrightarrow {{n_s}} d{S_s}} }  + \int\limits_{{V_{ijk}}} {\overrightarrow {{Q_V}} d{V_{ijk}}}  + \sum\limits_{s = 1}^6 {\int\limits_{{S_s}\left( {{V_{ijk}}} \right)} {\overline{\overline {{Q_S}}}  \cdot \overrightarrow {{n_s}} d{S_s}} } \end{array}\f$ \n
where the residual function \f$R\left({\overrightarrow U }\right)\f$ is the space operator. In order to compute this operator several different schemes are adopted for each different term of the operator.

For computing the numerical fluxes several different solvers are used. It is useful to distinguish between convective and diffusive fluxes:

For the convective fluxes \f$(\overline{\overline F}  - \overline{\overline {{Q_S}}})_{conv}  = \left[ {\begin{array}{*{20}{c}} {\rho \vec v}\\ {\rho \vec v\vec v + p\overline{\overline I } }\\ {(\rho E+p)\vec v } \end{array}} \right]\f$ Riemann Problem solvers are used (see \ref lib_riemann "Riemann module library"). High order approximation of numerical fluxes is obtained by means of WENO (Weighted Essentially Non Oscillatory) reconstruction, see \ref lib_weno "WENO module library".

For the diffusive fluxes \f${\left( {\overline{\overline F}  - \overline{\overline {{Q_S}}} } \right)_{diff}} = \left[ {\begin{array}{*{20}{c}} 0\\ { - \overline{\overline \tau } }\\ { - \overline{\overline \tau }  \cdot \vec v - k\overrightarrow \nabla  T} \end{array}} \right]\f$ a standard central second order approximation is used (based on Finite-Volume derivative approximation).

The time integration of the unsteady terms is computed by means of Runge-Kutta schemes, see \ref lib_runge_kutta "Runge-Kutta module library".

For more details see the Application Program Interface (API) pages of this documentation.

@section OtherModels Other Models

A set of sub-models are implemented in order to taking into account other physical phenomena.

@subsection MultiFluids Multi Fluids Model


@page DataStructure Data Structures

General Remarks on Data Structures

@section Considerations General Considerations

@off data structures are based on derived typed, dynamic allocatable objects.

The programming style is similar to Object Oriented Programming (OOP), but it is not a fully OOP code (Fortran 2003 is not fully compliant with OOP). The programming style is based on modules: each derived type has its own module containing the definition of type (object) and the procedures and local/global variables (and parameters) to handle it.  Moreover also the libraries of procedures (e.g. Runge-Kutta, WENO, mesh, fluid dynamics... procedures) are contained in modules.  Everywhere is possible the procedures are nested (OOP encapsulation) into the scope where they are necessary and they are not visible outside that scope. OOP dynamic dispatching by means of generic interfaces has been used when it was considered useful. A very basic OOP inheritance is present among some derived type (e.g. boundary conditions types, vector and tensor types, fluid dynamics ones).  Finally some OOP polymorphism is present.

The basic module that \b all other modules and programs must use it the \ref ir_precision "IR_Precision portable kinds library".  This module contains a parametric definition of portable (architecture independent) kinds for integers and reals representation. By means of this module the code uses always the same precision (with the same significant digits) for representing numbers independently on what architecture is using.

@section Hierarchy Data Structures Hierarchy

The underling numerical mesh adopted is a multi-blocks (presently without overlapping) structured grid. As a consequence the main data structures of @off have been divided into two different types:
- Global level data: are data related to all grid blocks; this kind of data has \em global pertinence; there is only one global level allocated variable (instance in OOP view);
- Block level data: are data related to only one block; this kind of data has \em block pertinence; each block has its own block level data (many instances how are the blocks).

The two above data structures are defined by two derived types contained into the libraries \ref data_type_global "Data_Type_Global" and \ref data_type_sblock "Data_Type_SBlock". The two derived types are:
- \ref data_type_global::type_global;
- \ref data_type_sblock::type_sblock.
Nested to these two derived types are many different kinds of data.

Details of the above types can be found in the <em>Application Program Interface</em> (API) documentation pages.

The block level data instance is allocated as a double indexed array:
@code
type(Type_Block), allocatable:: block(:,:) ! Dimensions [1:Nb,1:Nl] being Nb the number of blocks and Nl the number of grid levels
@endcode
where the inner index ranges in the blocks number while the outer index ranges in the grid (refinement) levels number. At today a bug afflicts multigrid algorithm thus the use of more different grid levels (Nl>1) is not useful.

Inside the block level data there are a lot of dynamically allocated 3D arrays. The use of nested dynamic data needs a careful treatment of the memory allocated. For those derived types (e.g. boundary conditions and fluid dynamics types) that have dynamic data specific procedures for allocating and freeing the memory have been provided (e.g. see \ref data_type_conservative::free_cons "Type Conservative freeing memory procedure").

According to the data hierarchy there are procedures that operate on some level of data structure and other on different level.  There are for example procedures operating on whole blocks of some grid level (e.g. see \ref lib_fluidynamic::solve_grl and \ref lib_fluidynamic::boundary_conditions) and other procedures operating on only one block at time (e.g. see \ref lib_fluidynamic::conservative2primitive).

@page FilesFormat Input Output Files Structures

General Remarks on Input/Output Files

@section IOFilesList List of IO Files

@off (and other auxiliary programs) uses a set of different input/output files:
- Input:
  - options files;
  - mesh files;
  - boundary conditions files;
  - initial conditions files;
- Output:
  - mesh files;
  - boundary conditions files;
  - solution files.

@section OptsFiles Option Files

@off (and IBM code) needs to be invoked with the path to the main option file:
@code
OFF off_options.dat
@endcode
where \em off_options.dat is an ascii file containing the main options of @off code. A template of this file is as following:
@code
------------------------------------------------------------------------------------------------------------------------------------
                                  OS TYPE
'uix'                                                        OS_type     = Operating System, UIX or WIN
                                  INPUT FILES
'./input/'                                                   Path_InPut  = input path
'solver.dat'                                                 File_Solver = name of solver options file
1                                                            Nl          = number of grid levels used
'sod'                                                        File_Mesh   = basename of mesh file
'sod'                                                        File_BC     = basename of boundary conditions file
'sod'                                                        File_Init   = basename of initial conditions file
                                  OUTPUT OPTIONS
'output/'                                                    Path_OutPut   = output path
'sod'                                                        Title_out     = title of output file
'.sol'                                                       Extension_out = extension of output file
10                                                           screen_out    = console refresh frequency
1                                                            sol_out       = actual solution output frequency
20                                                           restart_out   = restart output frequency
1                                                            probe_out     = probes output frequency
------------------------------------------------------------------------------------------------------------------------------------
@endcode
The options are self explained.

@note Note that the input and output directories must be terminated with '/' (or '\' for MS Windows architectures).

@note Note that lines 1, 2, 4, 11 and the last one are only comments, but they are necessary being records that are expected to be read from @off parser.

@subsection SolverFile Solver Options File
The options concerning with the different solvers are saved in an ascii file organized as following:

@code
------------------------------------------------------------------------------------------------------------------------------------
                                  SOLVER OPTIONS
'unsteady'                                                   Timing  = 'unsteady' simulation or steady one otherwise
0                                                            Nmax    = max number of iterations
0.15d0                                                       Tmax    = max time, ignored if Nmax>0
3                                                            sp_ord  = order of space convergence (order of state reconstruction)
2                                                            rk_ord  = order of time convergence (Runge-Kutta stages)
0.7d0                                                        CFL     = stability costant
0.01d0                                                       Res_Tol = residuals tollerance
                                  NON DIMENSIONAL NUMBERS
1.d0                                                         Re = Reynolds number
1.d0                                                         Fr = Froude   number
1.d0                                                         Pr = Prandtl  number
                                  REFERENCE VALUES
1.d0                                                         L0 = reference length
1.d0                                                         r0 = reference density
1.d0                                                         v0 = reference velocity
1.d0                                                         c0 = reference specific heat
------------------------------------------------------------------------------------------------------------------------------------
@endcode
The options are self explained.

@note Note that lines 1, 2, 10, 15 and the last one are only comments, but they are necessary being records that are expected to be read from @off parser.

@section DataFiles Data Files

Among the possible choices it has been chosen to don't use multi-blocks file: each data file contains only one block data. The author thinks that this one-block file template provides more flexibility and an easier file handling with MPI multi-processes simulations.

@subsection MeshFiles Mesh File
The data contained into mesh files and the format used are defined by \ref data_type_sblock::load_bmesh "loading" and \ref data_type_sblock::save_bmesh "saving" procedures. The mesh files are binary (unformatted) files with the following naming convention:
@code
mesh_base_name.g##.b###.geo
@endcode
where '.g##' is the grid level and '.b###' is the block number. For building such a name there is a specific \ref data_type_global::file_name "procedure".

@subsection BCFiles Boundary Conditions File
The data contained into boundary conditions files and the format used are defined by \ref data_type_sblock::load_bbc "loading" and \ref data_type_sblock::save_bbc "saving" procedures. The boundary conditions files are binary (unformatted) files with the following naming convention:
@code
bc_base_name.g##.b###.bco
@endcode
where '.g##' is the grid level and '.b###' is the block number. For building such a name there is a specific \ref data_type_global::file_name "procedure".

@subsection SolFiles Solution File
The data contained into solution files and the format used are defined by \ref data_type_sblock::load_bfluid "loading" and \ref data_type_sblock::save_bfluid "saving" procedures. The solution files are binary (unformatted) files with the following naming convention:
@code
sol_base_name.g##.b###.geo
@endcode
where '.g##' is the grid level and '.b###' is the block number. For building such a name there is a specific \ref data_type_global::file_name "procedure".

@page Compiling Compiling Instructions

Compiling Instructions

@section CompilersList List of Supported Compilers

@off has been developed on GNU/Linux architectures. Other OS are not supported (and in general there is no best alternative to GNU/Linux :-).

The codes provided (@off, IBM and POG) have been successfully compiled with the following compilers:

- GNU gfortran (version 4.7.0 or higher)
- Intel Fortran Compiler ifort (version 12.0 or higher)

Other compilers are not supported. For the Portland Group Compiler pgf it is known the presence of problems (and lack of performance due to the dynamic data nested into derived types).

@off can run over High Performance Computing facilities. It is parallelizated by means MPI paradigm for running over large distributed memory clusters and by means OpenMP paradigm for shared memory architectures.  Both the supported compilers provides OpenMP paradigm support. For MPI support a MPI implementation is necessary. @off has been successfully compiled by means OpenMPI and MPCH2 implementations. In particular the supported implementations are:
- OpenMPI (from version 1.4.5)  compiled against GNU gfortran;
- OpenMPI (from version 1.4.5)  compiled against Intel Fortran Compiler ifort;
- MPICH2 (from version 1.4.1)  compiled against GNU gfortran;
- MPICH2 (from version 1.4.1)  compiled against Intel Fortran Compiler ifort.

@section CompInst Compiling Instructions

The codes are constituted by several modules. Therefore there are many dependences. The most easy way to compile the code is to start with the provided makefile thus it is necessary that the system has "Make" program (preferably GNU make http://www.gnu.org/software/make/).

Examples of compiling are:
- @off code with default options:
  @code
  make
  @endcode
- parallel and performance-optimized @off:
  @code
  make DEBUG=no OPTIMIZE=yes OPENMP=yes MPI=yes
  @endcode
- auxiliaries codes:
  - IBM code with default options:
    @code
    make IBM
    @endcode
  - POG code with Tecplot binary library:
    @code
    make POG TECIO=yes
    @endcode
- building documentation:
  @code
  make doc
  @endcode
- cleaning the project directory:
  @code
  make cleanall
  @endcode

In the following subsections there are more details of each option and rule of the provided makefile.

@subsection Makefile

The provided makefile has several options. It has one rule that prints all options available and the default settings. Typing in the shell prompt: @code make help @endcode the following output will be printed:

@code
 Make options of OFF codes

 Compiler choice
  COMPILER=gnu   => GNU gfortran
  COMPILER=intel => Intel Fortran
  COMPILER=pgi   => Portland Group Fortran
  COMPILER=g95   => free g95
  COMPILER=gnu   => default

 Compiling options
  DEBUG=yes(no)    => on(off) debug                  (default yes)
  F03STD=yes(no)   => on(off) check standard fortran (default yes)
  OPTIMIZE=yes(no) => on(off) optimization           (default no)
  OPENMP=yes(no)   => on(off) OpenMP directives      (default no)
  MPI=yes(no)      => on(off) MPI    directives      (default no)

 Preprocessing options
  R16P=yes(no) => on(off) definition of real with "128 bit" (default no)
  NULi=yes(no) => on(off) nullify i direction (1D or 2D)    (default no)
  NULj=yes(no) => on(off) nullify j direction (1D or 2D)    (default no)
  NULk=yes(no) => on(off) nullify k direction (1D or 2D)    (default no)
  PPL=yes(no)  => on(off) Positivity Preserving Limiter     (default no)
  WENO=WENO/WENOZ/WENOM WENO algorithm (default WENO)
   WENO  => Original Jiang-Shu
   WENOZ => Improved Borges-Carmona-Costa-Don
   WENOM => Improved Henrick-Aslam-Powers
  RECV=RECVC/RECVP reconstruction variables type (default RECVC)
   RECVC => reconstruction in (local) characteristic variables
   RECVP => reconstruction in primitive variables
  RSU=HLLCb/HLLCc/HLLCp/HLLCt/HLLCz/EXA/PVL/TR/TS/APRS/ALFR/LF/LFz/ROE Riemann solver algorithm (default HLLCp)
   HLLCb => Approximate HLLC solver using BCLC waves speed estimation
   HLLCc => Approximate HLLC solver using CVL  waves speed estimation
   HLLCp => Approximate HLLC solver using PVL  waves speed estimation
   HLLCt => Approximate HLLC solver using TR   waves speed estimation
   HLLCz => Approximate HLLC solver using Z    waves speed estimation
   EXA   => Exact solver
   PVL   => Approximate PVL solver
   TR    => Approximate TR solver
   TS    => Approximate TS solver
   APRS  => Approximate APRS solver
   ALFR  => Approximate ALFR solver
   LFp   => Approximate Lax-Friedrichs solver using PVL waves speed estimation
   LFz   => Approximate Lax-Friedrichs solver using Z   waves speed estimation
   ROE   => Approximate Roe solver
  WS=WSu/WSup Waves Speed estimation algorithm (default WSup)
   WSu  => WavesSpeed14u  or WavesSpeed1234u  algorithm
   WSup => WavesSpeed14up or WavesSpeed1234up algorithm
  SMSW=SMSWz/SMSWliu/SMSWvanleer/SMSWvanalbada/SMSWharten smoothness switchg algorithm (default SMSWz)
   SMSWz         => Riemann-solver-like algorithm
   SMSWliu       => Liu algorithm
   SMSWvanleer   => van Leer slope limiter algorithm
   SMSWvanAlbada => van Albada slope limiter  algorithm
   SMSWharten    => Harten slope limiter algorithm
  HYBRID=NOHYBRID/HYBRID/HYBRIDC hybrid scheme (default NOHYBRID)
   NOHYBRID => no hybrid scheme
   HYBRID   => hybrid weno/weno_optimal scheme
   HYBRIDC  => hybrid weno/noweno_central scheme

 External libraries
  TECIO=yes(no) => on(off) Tecplot IO library linking (default no)

 Provided Rules
  Defualt rule => OFF
  help         => printing this help message
  OFF          => building OFF code
  IBM          => building IBM code
  POG          => building POG code
  cleanobj     => cleaning compiled object
  cleanmod     => cleaning .mod files
  cleanmsg     => cleaning make-log massage files
  cleanexe     => cleaning executable files
  clean        => running cleanobj, cleanmod and cleanmsg
  cleanall     => running clean and cleanexe
  tar          => creating a tar archive of the project
  doc          => building the documentation
@endcode

@subsection Mopts Makefile Options

The makefiles provides several options. These can be divided in make options (compiler choice and compiling options) and in pre-processing options. For the pre-processing options the C pre-process paradigm has been used.

The meaning of the options are:

- <b>Compiler choice:</b>
  - <b>COMPILER=gnu  </b> the codes is compiled using GNU gfortran compiler;
  - <b>COMPILER=intel</b> the codes is compiled using Intel Fortran compiler;
  - <b>COMPILER=pgi  </b> the codes is compiled using Portland Group Fortran compiler (<em>not yet supported</em>);
  - <b>COMPILER=g95  </b> the codes is compiled using free g95 compiler (<em>not yet supported</em>).
- <b>Compiling options:</b>
  - <b>DEBUG=yes(no)   </b> compilation with (or not) debug symbols;
  - <b>F03STD=yes(no)  </b> checking (or not) the compliance to Fortran standard 2003;
  - <b>OPTIMIZE=yes(no)</b> performance-optimized compilation (or not);
  - <b>OPENMP=yes(no)  </b> compilation with (or not) OpenMP directives;
  - <b>MPI=yes(no)     </b> compilation with (or not) MPI directives.
- <b>Preprocessing options:</b>
  - <b>R16P=yes(no)</b> activation (or not) support for real with "128 bit" (quadruple precision) representation;
  - <b>NULi=yes(no)</b> nullify i direction integration for 1D or 2D simulation;
  - <b>NULj=yes(no)</b> nullify j direction integration for 1D or 2D simulation;
  - <b>NULk=yes(no)</b> nullify k direction integration for 1D or 2D simulation;
  - <b>PPL=yes(no) </b> activation (or not) the Positivity Preserving Limiter for high order WENO scheme.
  - <b>WENO=WENO/WENOZ/WENOM WENO</b> WENO algorithm:
    - <b>WENO </b> original Jiang-Shu sheme;
    - <b>WENOZ</b> improved Borges-Carmona-Costa-Don scheme;
    - <b>WENOM</b> improved Henrick-Aslam-Powers scheme.
  - <b>RECV=RECVC/RECVP</b> high order WENO scheme reconstruction variables type:
    - <b>RECVC</b> reconstruction in (local) characteristic variables;
    - <b>RECVP</b> reconstruction in primitive variables.
  - <b>RSU=HLLCb/HLLCc/HLLCp/HLLCt/HLLCz/EXA/PVL/TR/TS/APRS/ALFR/LF/LFz/ROE</b> Riemann solver algorithm:
    - <b>HLLCb</b> approximate HLLC solver using BCLC waves speed estimation;
    - <b>HLLCc</b> approximate HLLC solver using CVL  waves speed estimation;
    - <b>HLLCp</b> approximate HLLC solver using PVL  waves speed estimation;
    - <b>HLLCt</b> approximate HLLC solver using TR   waves speed estimation;
    - <b>HLLCz</b> approximate HLLC solver using Z    waves speed estimation;
    - <b>EXA  </b> exact solver based on u-function iterative solution;
    - <b>PVL  </b> approximate PVL (Primitive Variables Linearization) solver;
    - <b>TR   </b> approximate TR (Two Rarefaction) solver;
    - <b>TS   </b> approximate TS (Two Shock) solver;
    - <b>APRS </b> approximate APRS (Adaptive PVL-TR-TS) solver;
    - <b>ALFR </b> approximate ALFR (Adaptive HLL-TR-TS) solver;
    - <b>LFp  </b> approximate local Lax-Friedrichs solver using PVL waves speed estimation;
    - <b>LFz  </b> approximate local Lax-Friedrichs solver using Z   waves speed estimation;
    - <b>ROE  </b> approximate Roe solver.
  - <b>WS=WSu/WSup</b> waves speed estimation algorithm:
    - <b>WSu </b> WavesSpeed14u  or WavesSpeed1234u  algorithm;
    - <b>WSup</b> WavesSpeed14up or WavesSpeed1234up algorithm;
  - <b>SMSW=SMSWz/SMSWliu/SMSWvanleer/SMSWvanalbada/SMSWharten</b> smoothness switchg algorithm:
    - <b>SMSWz        </b> Riemann-solver-like algorithm;
    - <b>SMSWliu      </b> Liu algorithm;
    - <b>SMSWvanleer  </b> van Leer slope limiter algorithm;
    - <b>SMSWvanAlbada</b> van Albada slope limiter  algorithm;
    - <b>SMSWharten   </b> Harten slope limiter algorithm.
  - <b>HYBRID=NOHYBRID/HYBRID/HYBRIDC</b>: hybrid (WENO/Central) scheme:
    - <b>NOHYBRID</b> no hybrid scheme;
    - <b>HYBRID  </b> hybrid weno/weno_optimal scheme;
    - <b>HYBRIDC </b> hybrid weno/noweno_central scheme.
@subsection Mrules Makefile Rules

The makefiles provides several rules. Their meanings are:

- <b>help    </b>: print help message in the shell prompt providing informations about the options and the rules available;
- <b>OFF     </b>: build OFF code according to the options chosen; the parallel options OPENMPI and MPI can be used;
- <b>IBM     </b>: build IBM code according to the options chosen; IBM has no parallel symbols thus activating the parallel options OPENMPI and MPI has no sense;
- <b>POG     </b>: build POG code according to the options chosen; POG has only OPENMPI symbols thus activating the parallel option MPI has no sense;
- <b>cleanobj</b>: clean compiled object file .o that are saved in obj directory by default;
- <b>cleanmod</b>: clean modules files .mod that are saved in obj directory by default;
- <b>cleanmsg</b>: clean log massage files of the make compilation that are "error_message" and "diagnostic_messages";
- <b>cleanexe</b>: clean executable files OFF, IBM and POG that are stored in ./ by default;
- <b>clean   </b>: execute cleanobj, cleanmod and cleanmsgr;
- <b>cleanall</b>: execute clean and cleanexe;
- <b>tar     </b>: create a tar archive containing input lib util and src directories and makefile;
- <b>doc     </b>: build the documentation (by means doxygen) in the directory doc/html.

@note
In order to build the documentation the following dependences must be satisfied:
- .doxigenconfig file must be in project main directory;
- doc directory with doc/fpp.sh, doc/mainpage.txt and doc/off-layout.xml files must be in project main directory;
- doxygen program must be installed (http://www.stack.nl/~dimitri/doxygen/).
All of the above pre-requisites are satisfied if the GitHub repository (https://github.com/szaghi/OFF) is cloned. \n
\n
@note
POG code can produce Tecplot binary files. To this aim the proprietary library tecio.a (or tecio64.a) from Tecplot Inc. company must be available (these libraries are shipped within Tecplot installer or they can be freely downloaded from http://download.tecplot.com/tecio/). By default the provided makefile search this libraries in ./lib/. The makefile try to recognized the type of OS (32 or 64 bit) by means of the <em>getconf LONG_BIT</em> shell command. According to the result of this command the library lib/32bit/tecio.a or lib/64bit/tecio64.a is passed to the linker. Note that for complete the building with these libraries the standard c++ library stdc++5 must be explicitly passed to the linker. The author prefers to put this last system library into the directories lib/32bit/ and lib/64bit/ to facilitate the makefile searching. The user must provide the necessary libraries and he must update the makefile with the correct paths.

@page Examples Examples of usage

Some Examples Description

The provided examples are contained into directory "examples". There are three types of examples:
 - \b 1D One dimensional test cases:
   - \subpage SOD "SOD" shock tube;
   - LAX shock tube.
 - \b 2D One dimensional test cases:
   - \subpage TWOD-RP "Two dimensional Riemann Problems";
   - Under expanded supersonic jet transient;
   - Double Mach reflections;
   - Shock corner diffraction.

@page Scalability Parallel Performance

Partial Assessment of Parallel Scalability on small HPC Cluster

For the (partial) assessment of \off performance let us consider the fifth test of Kurganov and Tadmor [\ref kurganov "1"] reported in the example section \ref kt-c05. Four different grid refinement levels have been considered for the whole \f$[0,1]\times[0,1]\f$ two dimensional domain ranging from \f$100\times100=10000\f$ (grid-1) to \f$800\times800=640000\f$ (grid-4) finite volumes. Each grid level is four times finer than the previous one.

Two different architectures have been used: 1) shared memory multicore workstation and 2) distributed memory cluster built up of shared memory multicore nodes. The shared memory workstation has two Esa core (12 cores) Intel(R) Xeon(R) X5650 at 2.67GHz with 24GB of DDR3 RAM at 1333MHz, whereas the cluster nodes have two Quad core (8 cores) Intel(R) Xeon(R) X5462 at 2.8GHz with 8GB of DDR3 RAM at 800MHz.

The numerical grid of the example section has been modified: each original block has been split into 4 new ones obtaining a 16 blocks grid. This allows to use up to 16 MPI processes increasing the tests matrix.

The assessment of \off scalability has been performed comparing the performance of the serial-compiled code with respect the parallel versions ones. In particular, defined \f$CPU_{SER}\f$ the CPU time used for the serial runs, the speedup is defined as \f$\frac{CPU}{CPU_{SER}}\f$. As a consequence, the ideal speedup is a linear function of the number of cores (processors, indicated as \f$\#CORE\f$ hereafter) used, ranging from \f$1\f$ (serial case) to \f$\frac{1}{\#CORE}\f$ (best case). In both the two architectures, three different parallel compilations have been considered: 1) pure OpenMP, 2) pure MPI and 3) hybrid OpenMP/MPI. All serial and parallel compilations have been performed using Intel Fortran compiler v. 13.1.1 and OpenMPI v. 1.6.4 (compiled with Intel Parallel suite, v. 13.1.1). Moreover, the -O3 (entailing vectorization) and -ipo optimization flags have been used.

@section SharedMemSpeedUP SpeedUp on Shared Memory Architecture

Figures \ref fig-shaspeedup050 "1", \ref fig-shaspeedup100 "2", \ref fig-shaspeedup200 "3" and \ref fig-shaspeedup400 "4" summarize the tests matrix performed on the shared memory architecture and the computed speedup. For the pure OpenMP runs all 11 possible configurations have been tested with the number of OpenMP threads used ranging in \f$[2,12]\f$. For the pure MPI and hybrid runs also some not balanced runs have performed. In particular, the tests with 3, 6 and 12 MPI processes are not balanced (having 16 blocks, the processes have different loads in these cases).

\off shows a good scalability on the shared memory architecture available. The pure MPI version exhibits a quasi-linear speedup for the balanced configurations, whereas OpenMP performance is lower. After a deep profiling some OpenMP issues have arisen: the current OpenMP (3.1) support for advanced Fortran 2003 features still lags behind and the most complex code sections had to be manually refactored to be parallelized. This clearly limits the performances which can be obtained especially when comparing to the pure MPI version. It is worth noting that, due to the memory bandwidth saturation the intra-node scaling is strongly limited up to 6 cores. As a consequence all the benchmarks using more than 6 cores show a non exciting speedup. As expected after the code profiling, the hybrid version scales better than the pure OpenMP.

<table border="0">
<tr><th>\anchor fig-shaspeedup050 \image html shaspeedup050x050.png "Figure 1: \off Scalability for the fifth test of Kurganov and Tadmor for grid level 1"</th><th> </th><th>\anchor fig-shaspeedup100 \image html shaspeedup100x100.png "Figure 2: \off Scalability for the fifth test of Kurganov and Tadmor for grid level 2"</th></tr>
<tr><th>\anchor fig-shaspeedup200 \image html shaspeedup200x200.png "Figure 3: \off Scalability for the fifth test of Kurganov and Tadmor for grid level 3"</th><th> </th><th>\anchor fig-shaspeedup400 \image html shaspeedup400x400.png "Figure 4: \off Scalability for the fifth test of Kurganov and Tadmor for grid level 4"</th></tr>
</table>

@section DistributedMemSpeedUP SpeedUp on Distributed Memory Architecture

The available distributed memory cluster has a total of 20 nodes, but only 8 nodes (for a total of 64 cores) have been available for these benchmarks. The numerical grid used has 16 blocks thus a maximum of 16 MPI processes can be executed in parallel. Due to the nodes RAM limitation (8GB) the serial run with the finest grid (grid-4) uses swap memory, thus invalidate the reference performance. As a consequence, for the cluster benchmarks the finest grid used is the third. Figures \ref fig-disspeedup050 "5" and \ref fig-disspeedup200 "6" summarizes the tests matrix performed on the cluster architecture and the computed speedup only for the grid levels 1 and 3. \off shows good scalability also on the distributed memory architecture. The inter-nodes performance are satisfactory as well as the intra-node scalability previously assessed on the shared memory workstation. The pure MPI and hybrid OpenMP/MPI codes are more efficient than the pure OpenMP version for the same overheads aforementioned. As expected, comparing the tests with different number of nodes and the number of cores used being equals, the best performance are obtained with the largest number of nodes because of the saturation of node memory bandwidth is minimized, e.g. comparing the MPI tests for grid-3 using 16 processors (with 2, 4 or 8 nodes) the best speedup is achieved with 8 nodes (\f$9.7\%\f$) while the 2 nodes test is the worst (\f$16.9\%\f$).

<table border="0">
<tr><th>\anchor fig-disspeedup050 \image html disspeedup050x050.png "Figure 5: \off Scalability for the fifth test of Kurganov and Tadmor for grid level 1"</th><th> </th><th>\anchor fig-disspeedup200 \image html disspeedup200x200.png "Figure 6: \off Scalability for the fifth test of Kurganov and Tadmor for grid level 3"</th></tr>
</table>

It is worth noting that, the reported benchmarks analysis constitutes only a partial assessment of the \off parallel scalability. In particular, tests on more powerful architectures (GIGA/PETA scale) with thousands to hundreds of thousands cores are necessary. On both the two architectures used the intra-node memory bandwidth limit has strongly affected the parallel scalability, more than the communications overheads. Indeed, this is a typical behavior of CFD codes, where the computations are orders of magnitude heavier than the communications.

*/
